Thank you for being willing to switch to this page to view our response. Also, thank you for your professional and highly constructive feedback.

First of all, the efficiency of our model indeed stems from the weight-sharing technique. Meanwhile, the plain Transformer technology can also effectively utilize the weight-sharing technique, as exemplified by ALBERT [1] . It not only improves efficiency but also effectively enhances the model's performance.

Similarly, we should not solely focus on efficiency. Instead, we strive to achieve an effective balance between efficiency and performance. Building on the work of predecessors and taking into account the heterogeneity of multimodal information, we successfully derived **Theory 1**: *Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs*. Based on this theory, we designed the Interlaced Mask mechanism to ensure the effectiveness of fusion. With the design of the Interlaced Mask, we can effectively integrate the weight-sharing mechanism, share the weights among multimodal sequences, and better fuse multimodal information. Meanwhile, we ensure high efficiency during the effective fusion process.

Regarding the current situation of plain Transformers, according to **Theory 1**, it is equivalent to the mathematical model we proposed in **Equation 1**. Its visualization is shown in **Figure 2, Part A**. In our theory, this may lead to information disorder (for details, please refer to **Section 8.1**), that is, it confounds the information of multimodal sequences. For example, when concatenating text (T), visual (V), and audio (A) information at the sequence dimension, in the case of a plain Transformer, during the Attention calculation part, the context information of T, V, and A will be confused into a single sequence. From our theoretical perspective, this will have a significant impact on performance, making this approach inadvisable.

Certainly, from your perspective, we noticed that in MERBench proposed in **W3**, the plain Attention mechanism outperforms the MulT paradigm on some datasets. Considering this issue, we have supplemented relevant experiments to complete our argumentation. Please refer to the response section of W3 for details. 

[1] Lan, Zhenzhong. "Albert: A lite bert for self-supervised learning of language representations." arXiv preprint arXiv:1909.11942 (2019).